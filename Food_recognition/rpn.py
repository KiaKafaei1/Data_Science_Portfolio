# -*- coding: utf-8 -*-
"""RPN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Xqz4TjqekiBACybMV0UGGSrgi4H8djF9
"""

import matplotlib
import numpy as np
import pandas as pd
import torch 
import torchvision
import torch.nn as nn
import torch.nn.functional as F 
from torchvision import transforms
import cv2 
from PIL import Image
import matplotlib.pyplot as plt
import glob
import os
from IPython.display import clear_output
from skimage.io import imread
from skimage.transform import resize
from google.colab import drive

def RPN(img0, bbox0, anchor_locations, anchor_labels, rpn_lambda):

  # run GPU .... 
  if(torch.cuda.is_available()):
      device = torch.device("cuda")
      print(device, torch.cuda.get_device_name(0))
  else:
      device= torch.device("cpu")
      print(device)

  #Resize the input images to (h=800, w=800)
  img = cv2.resize(img0, dsize=(800, 800), interpolation=cv2.INTER_CUBIC)

  # change the bounding box coordinates 
  Wratio = 800/img0.shape[1]
  Hratio = 800/img0.shape[0]
  ratioLst = [Hratio, Wratio, Hratio, Wratio]
  bbox = []
  for box in bbox0:
      box = [int(a * b) for a, b in zip(box, ratioLst)] 
      bbox.append(box)
  bbox = np.array(bbox)

  # List all the layers of VGG16
  # input can be smaller than 800 according to torchvision
  model = torchvision.models.vgg16(pretrained=True).to(device)
  fe = list(model.features)

  # collect layers with output feature map size (W, H) < 50
  dummy_img = torch.zeros((1, 3, 800, 800)).float() # test image array [1, 3, 800, 800] 

  #go through each layer and save output of layer into re_features
  req_features = []
  k = dummy_img.clone().to(device)
  for i in fe:
      k = i(k)
      if k.size()[2] < 800//16:   #800/16=50
          break
      req_features.append(i)
      out_channels = k.size()[1]

  # Convert this list into a Sequential module
  faster_rcnn_fe_extractor = nn.Sequential(*req_features)

  #feature extractor
  transform = transforms.Compose([transforms.ToTensor()]) # Defing PyTorch Transform
  imgTensor = transform(img).to(device) 
  imgTensor = imgTensor.unsqueeze(0)
  #use vgg16 network on our image
  out_map = faster_rcnn_fe_extractor(imgTensor)

  #RPN
  in_channels = 512 # depends on the output feature map. in vgg 16 it is equal to 512
  mid_channels = 512
  #default 9, men kan Ã¦ndres
  n_anchor = 9  # Number of anchors at each location

  conv1 = nn.Conv2d(in_channels, mid_channels, 3, 1, 1).to(device)
  conv1.weight.data.normal_(0, 0.01)
  conv1.bias.data.zero_()

  #the reason for n_anchor*4 is that it correspond to the position offset of each bounding box relative to the preset anchor box (1 region needs to predict 4 values of the prediction area Tx, Ty, Tw, Th) .
  #https://medium.com/@nabil.madali/demystifying-region-proposal-network-rpn-faa5a8fb8fce
  reg_layer = nn.Conv2d(mid_channels, n_anchor *4, 1, 1, 0).to(device)
  reg_layer.weight.data.normal_(0, 0.01)
  reg_layer.bias.data.zero_()

  #the reason for n_anchor*2 is that it correspond to the foreground and background probabilities of k regions at each pixel in Feature Map (1 region and 2 scores)
  cls_layer = nn.Conv2d(mid_channels, n_anchor *2, 1, 1, 0).to(device) ## I will be going to use softmax here. you can equally use sigmoid if u replace 2 with 1.
  cls_layer.weight.data.normal_(0, 0.01)
  cls_layer.bias.data.zero_()

  #apply definened layers on image
  x = conv1(out_map.to(device)) # out_map = faster_rcnn_fe_extractor(imgTensor)
  pred_anchor_locs = reg_layer(x)
  pred_cls_scores = cls_layer(x)

  # Rearrange arrays
  pred_anchor_locs = pred_anchor_locs.permute(0, 2, 3, 1).contiguous().view(1, -1, 4)
  pred_cls_scores = pred_cls_scores.permute(0, 2, 3, 1).contiguous()
  objectness_score = pred_cls_scores.view(1, 50, 50, 9, 2)[:, :, :, :, 1].contiguous().view(1, -1)
  pred_cls_scores  = pred_cls_scores.view(1, -1, 2)

  # Rearrange so input and outputs align
  rpn_loc = pred_anchor_locs[0]
  rpn_score = pred_cls_scores[0]
  gt_rpn_loc = torch.from_numpy(anchor_locations)
  gt_rpn_score = torch.from_numpy(anchor_labels)

  # Calculate cross entropy loss for classification
  rpn_cls_loss = F.cross_entropy(rpn_score, gt_rpn_score.long().to(device), ignore_index = -1)

  # Find the L1 regression loss for the bounding boxes
  # Find bounding boxes with positive label
  pos = gt_rpn_score > 0
  mask = pos.unsqueeze(1).expand_as(rpn_loc)

  # Take the bounding boxes with positive label
  mask_loc_preds = rpn_loc[mask].view(-1, 4)
  mask_loc_targets = gt_rpn_loc[mask].view(-1, 4)

  # Apply regression loss to the found bounding boxes
  x = torch.abs(mask_loc_targets.to(device) - mask_loc_preds.to(device))
  rpn_loc_loss = ((x < 1).float() * 0.5 * x**2) + ((x >= 1).float() * (x-0.5))

  # Combine the classification and regression loss to get the total loss for RPN
  # Define hyperparameter lambda
  N_reg = (gt_rpn_score >0).float().sum()
  rpn_loc_loss = rpn_loc_loss.sum() / N_reg
  rpn_loss = rpn_cls_loss + (rpn_lambda * rpn_loc_loss)
  return pred_anchor_locs, objectness_score, out_map, rpn_loc_loss, rpn_cls_loss, rpn_loss

